{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilingual BERT Infomax with image HE High RAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPy5uglhqQYQ",
        "colab_type": "text"
      },
      "source": [
        "*   **SEED = 42** \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1eNCKuMqV_S",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyZMo5A8t2GK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3hC_qnvt5SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZKm3PjqfKw",
        "colab_type": "text"
      },
      "source": [
        "# BERT Pretrained Model Download "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQsRqD2Ht7JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuKw9eWfrpv2",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loading (Text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs-LArkft9Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = '/content/gdrive/My Drive/COLING 2020/dataset (Cleaned).csv'\n",
        "df = pd.read_csv(file, sep = '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmW01xx0t-zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(df, test_size=0.1,random_state = SEED,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ZPFUj2t-86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_eng_hindi(a):\n",
        "  b_ = list(a['gold_label'])\n",
        "  lab = []\n",
        "  \"\"\"\n",
        "  lab  = []\n",
        "  for i in b_:\n",
        "    lab.append(i-1)\n",
        "  \"\"\"\n",
        "  for i in b_:\n",
        "    if i=='contradiction':\n",
        "        lab.append(0)\n",
        "        \n",
        "    elif i=='neutral':\n",
        "        lab.append(1)\n",
        "    elif i== 'entailment':\n",
        "        lab.append(2)\n",
        "    else:\n",
        "        lab.append(3)\n",
        "  sentence_1 = list(a['english_premise'])\n",
        "  sentence_2 = list(a['hypo_hindi'])\n",
        "  raw_data_train = {'sentence1_eng': sentence_1, \n",
        "              'sentence2_hindi': sentence_2,\n",
        "          'label': lab}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['sentence1_eng','sentence2_hindi','label'])\n",
        "  return df\n",
        "\n",
        "def get_data_hindi_eng(a):\n",
        "  b_ = list(a['gold_label'])\n",
        "  lab = []\n",
        "  \"\"\"\n",
        "  lab  = []\n",
        "  for i in b_:\n",
        "    lab.append(i-1)\n",
        "  \"\"\"\n",
        "  for i in b_:\n",
        "    if i=='contradiction':\n",
        "        lab.append(0)\n",
        "        \n",
        "    elif i=='neutral':\n",
        "        lab.append(1)\n",
        "    elif i== 'entailment':\n",
        "        lab.append(2)\n",
        "    else:\n",
        "        lab.append(3)\n",
        "  sentence_1 = list(a['premise_hindi'])\n",
        "  sentence_2 = list(a['english_hypo'])\n",
        "  raw_data_train = {'sentence1_hindi': sentence_1, \n",
        "              'sentence2_eng': sentence_2,\n",
        "          'label': lab}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['sentence1_hindi','sentence2_eng','label'])\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzGehz_Rt_FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_eng_hindi = get_data_eng_hindi(train)\n",
        "train_hindi_eng = get_data_hindi_eng(train)\n",
        "\n",
        "test_eng_hindi = get_data_eng_hindi(test)\n",
        "test_hindi_eng = get_data_hindi_eng(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnhRL6C9t_RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_eng_hindi[0:3])\n",
        "print(train_hindi_eng[0:3])\n",
        "print(test_eng_hindi[0:3])\n",
        "print(test_hindi_eng[0:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSgcc6Xrrp6y",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loading (Image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGtoqnUXuGa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#Flickr30K Dataset Attach and Image Preprocess\n",
        "\n",
        "uploaded = files.upload() #Upload the API Key for Kaggle (Kaggle.json)\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d hsankesara/flickr-image-dataset\n",
        "!unzip \"/content/flickr-image-dataset.zip\"\n",
        "\n",
        "file = '/content/gdrive/My Drive/COLING 2020/dataset.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# Testing for Proper loading of Image\n",
        "test_caption = list(df['captionID'])[10][:-2]\n",
        "image_file = \"/content/flickr30k_images/flickr30k_images/\"\n",
        "img = Image.open(image_file + test_caption)\n",
        "plt.imshow(img)\n",
        "\n",
        "img_lib = \"/content/flickr30k_images/flickr30k_images/\"\n",
        "images = list(df['captionID'])\n",
        "for i in range(len(images)):\n",
        "  images[i] = images[i][:-2]   #Last 2 characters contains non relevant hash-values\n",
        "\n",
        "image_height,image_width = 100,100  #Optimal for RAM Usage\n",
        "\n",
        "image_array = np.zeros((36072,image_height,image_width,3), dtype = np.float32)  #Because 146 error entries\n",
        "index = 0\n",
        "errors = []\n",
        "for i in images:\n",
        "  try:\n",
        "    print(\"Processing File: \"+i)\n",
        "    img = Image.open(img_lib + i)\n",
        "    img = img.resize((image_height,image_width))\n",
        "    img = np.asarray(img, dtype = np.float32)\n",
        "    image_array[index] = img\n",
        "    index += 1\n",
        "  except:\n",
        "    index += 1\n",
        "    print(\"Error at Index: \"+ str(index))\n",
        "    errors.append(index)\n",
        "\n",
        "np.array(errors).dump(open('Image Error Indices.npy', 'wb'))    #Useful for Sentence Deletion or Manual Image Insertion\n",
        "images_array = train_images_array/255\n",
        "train_imgages, test_images = train_test_split(images_array, test_size=0.1,random_state = SEED, shuffle = True)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# Image Numpy File loading Already Resized and Preprocessed\n",
        "file = '/content/gdrive/My Drive/COLING 2020/image_array_150_150.npy'\n",
        "images_array = np.load(file)\n",
        "\n",
        "# Train Test Split for Input in SOTA \n",
        "train_images, test_images = train_test_split(images_array, test_size=0.1,random_state = SEED, shuffle = True)\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck6IUQLWrqBZ",
        "colab_type": "text"
      },
      "source": [
        "# Using SOTA Image DNNs for extracting pretrained features\n",
        "\n",
        "*   VGG19\n",
        "*   NASNet Large\n",
        "*   InceptionResnetV2 [InceptionResnetV2 Paper](https://arxiv.org/abs/1602.07261)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pQfg1EguMFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Planning to use NasNetLarge\n",
        "from tensorflow.keras import applications\n",
        "\n",
        "#Image_Model = applications.NASNetLarge(include_top = False, input_shape = (100, 100, 3), weights = \"imagenet\") \n",
        "#Image_Model = applications.VGG19(include_top = False, input_shape = (100, 100, 3), weights = \"imagenet\")\n",
        "Image_Model = applications.InceptionResNetV2(include_top = False, input_shape = (150, 150, 3), weights = \"imagenet\")\n",
        "x = Image_Model.output\n",
        "img_features = tf.keras.layers.Flatten()(x)\n",
        "Image_model_final = tf.keras.Model(Image_Model.input, img_features)\n",
        "\n",
        "train_img_features = Image_model_final.predict(train_images)\n",
        "test_img_features = Image_model_final.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2Fg6v7RUo82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Freeing up Memory by setting reference variable to None after they are used\n",
        "images_array = None\n",
        "train_images = None\n",
        "test_images = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmRBelV2rqE6",
        "colab_type": "text"
      },
      "source": [
        "# Changing Raw Inpput to Bert Readable Inputs (Train and Test) Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFaxQ5KuO3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list = [0,1,2,3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQZElAGAuS-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_InputExamples_eng = train_eng_hindi.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x['sentence1_eng'], \n",
        "                                                                   text_b = x['sentence2_hindi'], \n",
        "                                                                   label = x['label']), axis = 1)\n",
        "train_InputExamples_hindi = train_hindi_eng.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x['sentence1_hindi'], \n",
        "                                                                   text_b = x['sentence2_eng'], \n",
        "                                                                   label = x['label']), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ixiiZuuTEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_InputExamples_eng = test_eng_hindi.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x['sentence1_eng'], \n",
        "                                                                   text_b = x['sentence2_hindi'], \n",
        "                                                                   label = x['label']), axis = 1)\n",
        "test_InputExamples_hindi = test_hindi_eng.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x['sentence1_hindi'], \n",
        "                                                                   text_b = x['sentence2_eng'], \n",
        "                                                                   label = x['label']), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMUbG7jwuTKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file = \"multi_cased_L-12_H-768_A-12/vocab.txt\"\n",
        "def create_tokenizer_from_hub_module():\n",
        " \n",
        "  return bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=True)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vZZPEg7rqJ7",
        "colab_type": "text"
      },
      "source": [
        "# Checking BERT Hindi and English Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaOnSXIIuZBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tokenizer.tokenize(\"how are you\"))\n",
        "print(tokenizer.tokenize(\"एक आदमी गोरा सिर वाली महिला से बात कर रहा है।\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vsA-bBYrqUY",
        "colab_type": "text"
      },
      "source": [
        "# Changing Raw Inpput to Bert Readable Inputs (Train and Test) Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH764RQWubgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features_eng = bert.run_classifier.convert_examples_to_features(train_InputExamples_eng, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "train_features_hindi = bert.run_classifier.convert_examples_to_features(train_InputExamples_hindi, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MudlwF6udaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "test_features_eng = bert.run_classifier.convert_examples_to_features(test_InputExamples_eng, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features_hindi = bert.run_classifier.convert_examples_to_features(test_InputExamples_hindi, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqiIMgJrq-w",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-BERT Custom Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVk6Z7w4uo8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels,output_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels,hidden_context) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "       \n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels, \"hidden_context\": hidden_context})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBkIEe6Hrq79",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-BERT Custom Model Definition with **Image Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PibwhKRPupyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_img(img_features,bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "  old_size = img_features.shape[-1].value\n",
        "\n",
        "  #output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size*2], initializer=tf.truncated_normal_initializer(stddev=0.02))  #Concatenate\n",
        "  output_bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "  output_weights_img = tf.get_variable(\"output_weights_img\", [hidden_size,old_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_bias_img = tf.get_variable(\"output_bias_img\", [hidden_size], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    img_features = tf.matmul(img_features, output_weights_img, transpose_b=True)\n",
        "    img_features = tf.nn.bias_add(img_features, output_bias_img)\n",
        "    img_features = tf.nn.relu(img_features)\n",
        "\n",
        "    #output_layer = tf.math.multiply(output_layer,img_features)\n",
        "    output_layer = tf.keras.layers.concatenate([output_layer,img_features])\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels,output_layer)\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder_img(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    img_features = features[\"img_features\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels,hidden_context) = create_model_img(\n",
        "        img_features, bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "       \n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels, \"hidden_context\": hidden_context})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxarj7wkrq5j",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-Progressive-BERT Custom Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESrCwHX2usTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_progressive(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings,hidden_context):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "\n",
        "    output_layer_probs = tf.nn.softmax(output_layer,axis = -1)\n",
        "    #loss = y_true * log(y_true / y_pred)\n",
        "    hidden_context = tf.nn.softmax(hidden_context,axis = -1)\n",
        "    per_example_kd_loss = tf.keras.losses.KLD(hidden_context,output_layer_probs)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "\n",
        "    kd_loss_weight = 0.2 #hyperparameter\n",
        "    per_example_kd_loss = kd_loss_weight*per_example_kd_loss\n",
        "\n",
        "    per_example_loss += per_example_kd_loss\n",
        "\n",
        "    \n",
        "\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder_progressive(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    hidden_context = features[\"hidden_context\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels) = create_model_progressive(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings,hidden_context)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya64btz5rq2y",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-Progressive-BERT Custom Model Definition with **Image Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZThLldNPuu88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_progressive_img(img_features,bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings,hidden_context):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "  old_size = img_features.shape[-1].value\n",
        "\n",
        "  #output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_weights = tf.get_variable(\"output_weights\", [num_labels, old_size], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_bias = tf.get_variable(\"output_bias\", [num_labels],initializer=tf.zeros_initializer())\n",
        "  output_weights_img = tf.get_variable(\"output_weights_img\", [hidden_size,old_size],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "  output_bias_img = tf.get_variable(\"output_bias_img\", [hidden_size],initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    img_features = tf.matmul(img_features, output_weights_img, transpose_b=True)\n",
        "    img_features = tf.nn.bias_add(img_features, output_bias_img)\n",
        "    img_features = tf.nn.relu(img_features)\n",
        "\n",
        "    #output_layer  = tf.math.multiply(img_features,output_layer)\n",
        "    output_layer = tf.keras.layers.concatenate([output_layer,img_features])\n",
        "    output_layer_probs = tf.nn.softmax(output_layer,axis = -1)\n",
        "    #loss = y_true * log(y_true / y_pred)\n",
        "    hidden_context = tf.nn.softmax(hidden_context,axis = -1)\n",
        "    per_example_kd_loss = tf.keras.losses.KLD(hidden_context,output_layer_probs)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "\n",
        "    kd_loss_weight = 0.2 #hyperparameter\n",
        "    per_example_kd_loss = kd_loss_weight*per_example_kd_loss\n",
        "\n",
        "    per_example_loss += per_example_kd_loss\n",
        "\n",
        "    \n",
        "\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder_img_progressive(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    hidden_context = features[\"hidden_context\"]\n",
        "    img_features = features[\"img_features\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels) = create_model_progressive_img(\n",
        "        img_features,bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings,hidden_context)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uNc6dcsrqia",
        "colab_type": "text"
      },
      "source": [
        "# Input Functions\n",
        "\n",
        "1.   CLTE-BERT\n",
        "2.   CLTE-BERT with Image\n",
        "3.   CLTE-BERT-Progressive with Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n8-XpQ5uxFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features, hidden_context,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape = hidden_context.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"hidden_context\":\n",
        "            tf.constant(hidden_context, shape = [num_examples,hidden_shape], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "\n",
        "def input_fn_builder_img(img_features,features,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape_img = img_features.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"img_features\":\n",
        "            tf.constant(img_features, shape = [num_examples,hidden_shape_img], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def input_fn_builder_pr_img(img_features,features,hidden_context,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape_img = img_features.shape[-1]\n",
        "    hidden_shape = hidden_context.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"img_features\":\n",
        "            tf.constant(img_features, shape = [num_examples,hidden_shape_img], dtype = tf.float32),\n",
        "\n",
        "        \"hidden_context\":\n",
        "            tf.constant(hidden_context, shape = [num_examples,hidden_shape], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJqVRXAurqgj",
        "colab_type": "text"
      },
      "source": [
        "# Trainer Functions for BERT (With and Without Image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trvxhdrguz2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 10 # Number of Training Epochs \n",
        "\n",
        "\n",
        "def train(output_dir,input_fn,input_fn_builder_progressive = False,hidden_context = None):\n",
        "  CONFIG_FILE = \"multi_cased_L-12_H-768_A-12/bert_config.json\"\n",
        "  INIT_CHECKPOINT = \"multi_cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "\n",
        "  BATCH_SIZE = 28\n",
        "  LEARNING_RATE = 2e-5\n",
        "  NUM_TRAIN_EPOCHS = Epochs\n",
        "  # Warmup is a period of time where hte learning rate \n",
        "  # is small and gradually increases--usually helps training.\n",
        "  WARMUP_PROPORTION = 0.1\n",
        "  # Model configs\n",
        "  SAVE_CHECKPOINTS_STEPS = 6000\n",
        "  SAVE_SUMMARY_STEPS = 100\n",
        "  OUTPUT_DIR = output_dir\n",
        "  # Compute # train and warmup steps from batch size\n",
        "  num_train_steps = int(len(input_fn) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "  print(num_train_steps)\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "  # Specify outpit directory and number of checkpoint steps to save\n",
        "  if input_fn_builder_progressive==False:\n",
        "  \n",
        "\n",
        "\n",
        "    model_fn = model_fn_builder(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "  \n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=input_fn,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "\n",
        "  else:\n",
        "\n",
        "    model_fn_pr = model_fn_builder_progressive(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn_pr,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "    train_input_fn = input_fn_builder(\n",
        "        features=input_fn,\n",
        "        hidden_context=hidden_context,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "  print(f'Beginning Training!')\n",
        "  %timeit\n",
        "\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  return estimator\n",
        "\n",
        "\n",
        "def train_img(img_features,output_dir,input_fn,input_fn_builder_progressive = False,hidden_context = None):\n",
        "  CONFIG_FILE = \"multi_cased_L-12_H-768_A-12/bert_config.json\"\n",
        "  INIT_CHECKPOINT = \"multi_cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "\n",
        "  BATCH_SIZE = 28\n",
        "  LEARNING_RATE = 2e-5\n",
        "  NUM_TRAIN_EPOCHS = Epochs              \n",
        "  # Warmup is a period of time where hte learning rate \n",
        "  # is small and gradually increases--usually helps training.\n",
        "  WARMUP_PROPORTION = 0.1\n",
        "  # Model configs\n",
        "  SAVE_CHECKPOINTS_STEPS = 6000\n",
        "  SAVE_SUMMARY_STEPS = 100\n",
        "  OUTPUT_DIR = output_dir\n",
        "  # Compute # train and warmup steps from batch size\n",
        "  num_train_steps = int(len(input_fn) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "  print(num_train_steps)\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "  # Specify outpit directory and number of checkpoint steps to save\n",
        "  if input_fn_builder_progressive==False:\n",
        "  \n",
        "\n",
        "\n",
        "    model_fn = model_fn_builder_img(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "  \n",
        "    train_input_fn = input_fn_builder_img(\n",
        "        img_features = img_features,\n",
        "        features=input_fn,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "\n",
        "  else:\n",
        "\n",
        "    model_fn_pr = model_fn_builder_img_progressive(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn_pr,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "    train_input_fn = input_fn_builder_pr_img(\n",
        "        img_features = img_features,\n",
        "        features=input_fn,\n",
        "        hidden_context=hidden_context,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "  print(f'Beginning Training!')\n",
        "  %timeit\n",
        "\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNzleH_urqeW",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Functions for BERT (With and Without Image)\n",
        "\n",
        "*   CTX = 0 for English Premise and Hindi Hypothesis\n",
        "*   CTX = 1 for Hindi Premise and English Hypothesis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbhGSEfgu2DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "def evaluate_and_get_hidden_context(estimator,input_fn_for_test,input_fn_for_hidden,is_progressive = False,hidden_context=None):\n",
        "  MAX_SEQ_LENGTH = 128\n",
        " \n",
        "  if not is_progressive:\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "     \n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    hidden_input_fn = run_classifier.input_fn_builder(\n",
        "        features=input_fn_for_hidden,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    res = estimator.predict(hidden_input_fn)\n",
        "    hidden_context = []\n",
        "    for i in res:\n",
        "      hidden_context.append(i[\"hidden_context\"])\n",
        "    hidden_context = np.array(hidden_context)\n",
        "    return hidden_context\n",
        "  else:\n",
        "    test_input_fn = input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      hidden_context=hidden_context,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "'''\n",
        "\n",
        "def evaluate_and_get_hidden_context(ctx,estimator,input_fn_for_test,input_fn_for_hidden,is_progressive = False,hidden_context=None):\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "  if not is_progressive:\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    actual_labels = []\n",
        "    if ctx ==0:\n",
        "      for i in test_eng_hindi['label']:\n",
        "        actual_labels.append(i)\n",
        "    elif ctx==1:\n",
        "      for i in test_hindi_eng['label']:\n",
        "        actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    hidden_input_fn = run_classifier.input_fn_builder(\n",
        "        features=input_fn_for_hidden,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    res = estimator.predict(hidden_input_fn)\n",
        "    hidden_context = []\n",
        "    for i in res:\n",
        "      hidden_context.append(i[\"hidden_context\"])\n",
        "    hidden_context = np.array(hidden_context)\n",
        "    return hidden_context,actual_labels,predicted_labels\n",
        "  else:\n",
        "    test_input_fn = input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      hidden_context=hidden_context,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    actual_labels = []\n",
        "    if ctx ==0:\n",
        "      for i in test_eng_hindi['label']:\n",
        "        actual_labels.append(i)\n",
        "    elif ctx==1:\n",
        "      for i in test_hindi_eng['label']:\n",
        "        actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "    return actual_labels,predicted_labels\n",
        "\n",
        "\n",
        "\n",
        "#IMG\n",
        "def evaluate_and_get_hidden_context_img(ctx,img_features_for_test,img_features,estimator,input_fn_for_test,input_fn_for_hidden,is_progressive = False,hidden_context=None):\n",
        "  MAX_SEQ_LENGTH = 128\n",
        " \n",
        "  if not is_progressive:\n",
        "    test_input_fn = input_fn_builder_img(\n",
        "      features=input_fn_for_test,\n",
        "      img_features = img_features_for_test,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    actual_labels = []\n",
        "    if ctx ==0:\n",
        "      for i in test_eng_hindi['label']:\n",
        "        actual_labels.append(i)\n",
        "    elif ctx==1:\n",
        "      for i in test_hindi_eng['label']:\n",
        "        actual_labels.append(i)\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    hidden_input_fn = input_fn_builder_img(\n",
        "        features=input_fn_for_hidden,\n",
        "        img_features = img_features,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    res = estimator.predict(hidden_input_fn)\n",
        "    hidden_context = []\n",
        "    for i in res:\n",
        "      hidden_context.append(i[\"hidden_context\"])\n",
        "    hidden_context = np.array(hidden_context)\n",
        "    return hidden_context, actual_labels,predicted_labels\n",
        "  else:\n",
        "    test_input_fn = input_fn_builder_pr_img(\n",
        "      img_features = img_features_for_test,\n",
        "      features=input_fn_for_test,\n",
        "      hidden_context=hidden_context,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    actual_labels = []\n",
        "    if ctx ==0:\n",
        "      for i in test_eng_hindi['label']:\n",
        "        actual_labels.append(i)\n",
        "    elif ctx==1:\n",
        "      for i in test_hindi_eng['label']:\n",
        "        actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "    return actual_labels,predicted_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgByV60krqcD",
        "colab_type": "text"
      },
      "source": [
        "# Training for English Premise and Hindi Hypothesis\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUBrZX9Au4O1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = train('out_dir_train_eng',train_features_eng,input_fn_builder_progressive = False,hidden_context = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d_cFV20rqZs",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for English Premis and Hindi Hypothesis\n",
        "\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZxAAk2eu7FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_eng, act_lab, pred_lab = evaluate_and_get_hidden_context(0,estimator,input_fn_for_test = test_features_eng,input_fn_for_hidden = train_features_eng,is_progressive = False)\n",
        "\n",
        "np.array(act_lab).dump(open('EH_Actual_labels_Normal_Imageless.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('EH_Predicted_labels_Normal_Imageless.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('EH_Actual_labels_Normal_Imageless.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('EH_Predicted_labels_Normal_Imageless.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ6Ca80E91vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(hidden_context_eng).dump(open('Hidden_Context_English_Normal.npy', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-8d8VPrqX-",
        "colab_type": "text"
      },
      "source": [
        "# Training for Hindi Premise and English Hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkc3Z6qhu9G0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = train('out_dir_train_hindi',train_features_hindi,input_fn_builder_progressive = False,hidden_context = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXg8xgK3rqIP",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for Hindi Premis and English Hypothesis\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WCIjhQ8u_iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_hindi, act_lab, pred_lab = evaluate_and_get_hidden_context(1,estimator,input_fn_for_test = test_features_hindi,input_fn_for_hidden = train_features_hindi,is_progressive = False)\n",
        "\n",
        "np.array(act_lab).dump(open('HE_Actual_labels_Normal_Imageless.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('HE_Predicted_labels_Normal_Imageless.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('HE_Actual_labels_Normal_Imageless.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('HE_Predicted_labels_Normal_Imageless.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTO-WEAK9pfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(hidden_context_hindi).dump(open('Hidden_Context_Hindi_Normal.npy', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjdlPxj0s1wD",
        "colab_type": "text"
      },
      "source": [
        "# Progressive Training on English Premise and Hindi Hypothesis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFQdoMANvEq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_hindi = np.load('Hidden_Context_Hindi_Normal.npy', allow_pickle=True)\n",
        "estimator = train('out_dir_train_eng_pro',train_features_eng,input_fn_builder_progressive = True,hidden_context = hidden_context_hindi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMrJfOUys1tx",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for English Premis and Hindi Hypothesis (Progressive Variant)\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "takvo4rVvGw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_batch_size = 50\n",
        "dummy = np.random.randn(Test_batch_size,768)\n",
        "act_lab, pred_lab = evaluate_and_get_hidden_context(0,estimator,input_fn_for_test = test_features_eng,input_fn_for_hidden = train_features_eng,is_progressive = True,hidden_context=dummy)\n",
        "\n",
        "np.array(act_lab).dump(open('EH_Actual_labels_Progressive_Imageless.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('EH_Predicted_labels_Progressive_Imageless.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('EH_Actual_labels_Progressive_Imageless.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('EH_Predicted_labels_Progressive_Imageless.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxceP_3gs1rS",
        "colab_type": "text"
      },
      "source": [
        "# Progressive Training on Hindi Premise and English Hypothesis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuGgv5tYvKEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_eng = np.load('Hidden_Context_English_Normal.npy', allow_pickle=True)\n",
        "estimator = train('out_dir_train_hindi_pro',train_features_hindi,input_fn_builder_progressive = True,hidden_context = hidden_context_eng)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx-U9Xcks1pC",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for Hindi Premis and English Hypothesis (Progressive Variant)\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeyqesMxvM4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_batch_size = 50\n",
        "dummy = np.random.randn(Test_batch_size,768)\n",
        "act_lab, pred_lab = evaluate_and_get_hidden_context(1,estimator,input_fn_for_test = test_features_hindi,input_fn_for_hidden = train_features_hindi,is_progressive = True,hidden_context=dummy)\n",
        "\n",
        "np.array(act_lab).dump(open('HE_Actual_labels_Progressive_Imageless.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('HE_Predicted_labels_Progressive_Imageless.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('HE_Actual_labels_Progressive_Imageless.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('HE_Predicted_labels_Progressive_Imageless.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOyqgyDes1m5",
        "colab_type": "text"
      },
      "source": [
        "# Training for English Premise and Hindi Hypothesis (Image Included)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8e5gxaKvSEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = train_img(train_img_features,'out_dir_train_eng_img',train_features_eng,input_fn_builder_progressive = False,hidden_context = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOy7qpzvs1k5",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for English Premis and Hindi Hypothesis (Image Included)\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC020yOpvUlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_eng_img, act_lab, pred_lab = evaluate_and_get_hidden_context_img(0,test_img_features,train_img_features,estimator,input_fn_for_test = test_features_eng,input_fn_for_hidden = train_features_eng,is_progressive = False)\n",
        "\n",
        "np.array(act_lab).dump(open('EH_Actual_labels_Normal_Image.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('EH_Predicted_labels_Normal_Image.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('EH_Actual_labels_Normal_Image.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('EH_Predicted_labels_Normal_Image.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l-pK9NL8cbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(hidden_context_eng_img).dump(open('/content/gdrive/My Drive/COLING 2020/Hidden_Context_English_Image_10.npy', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8owzp2uls1i4",
        "colab_type": "text"
      },
      "source": [
        "# Training for Hindi Premise and English Hypothesis (Image Included)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdJDLGktvWKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = train_img(train_img_features,'out_dir_train_hindi_img',train_features_hindi,input_fn_builder_progressive = False,hidden_context = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK-jYTVXs1gg",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for Hindi Premis and English Hypothesis (Image Included)\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLq9M8q3vZv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_hindi_img, act_lab, pred_lab = evaluate_and_get_hidden_context_img(1,test_img_features,train_img_features,estimator,input_fn_for_test = test_features_hindi,input_fn_for_hidden = train_features_hindi,is_progressive = False)\n",
        "\n",
        "np.array(act_lab).dump(open('HE_Actual_labels_Normal_Image_Concat.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('HE_Predicted_labels_Normal_Image_Concat.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('HE_Actual_labels_Normal_Image_Concat.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('HE_Predicted_labels_Normal_Image_Concat.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep4PfIeF9Z--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(hidden_context_hindi_img).dump(open('/content/gdrive/My Drive/COLING 2020/Hidden_Context_Hindi_Image_Concat.npy', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7co-B9Is1eJ",
        "colab_type": "text"
      },
      "source": [
        "# Progressive Training for English Premise and Hindi Hypothesis (Image Included)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKdUVTzGvat4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_hindi_img = np.load('/content/gdrive/My Drive/COLING 2020/Hidden_Context_Hindi_Image_Concat.npy', allow_pickle=True)\n",
        "estimator = train_img(train_img_features,'out_dir_train_eng_pro_img',train_features_eng,input_fn_builder_progressive = True, hidden_context = hidden_context_hindi_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygJUXSL4s1bl",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation and Hidden Context generation for English Premis and Hindi Hypothesis (Image Included) (Progressive Variant)\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b5Uqutevdx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_batch_size = 3593    # Test Split Size\n",
        "\n",
        "dummy = np.random.randn(Test_batch_size,1536)\n",
        "#test_img_features = np.random.randn(50,1024)\n",
        "#evaluate_img_features = np.random.randn(450,1024)\n",
        "act_lab, pred_lab = evaluate_and_get_hidden_context_img(0,test_img_features,train_img_features,estimator,input_fn_for_test = test_features_eng,input_fn_for_hidden = train_features_eng,is_progressive = True,hidden_context=dummy)\n",
        "\n",
        "np.array(act_lab).dump(open('EH_Actual_labels_Progressive_Image_10.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('EH_Predicted_labels_Progressive_Image_10.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('EH_Actual_labels_Progressive_Image_10.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('EH_Predicted_labels_Progressive_Image_10.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW2vRMk9tcM4",
        "colab_type": "text"
      },
      "source": [
        "# Progressive Training for Hindi Premise and English Hypothesis (Image Included)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms9vQ59ZvtWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_context_eng_img = np.load('/content/gdrive/My Drive/COLING 2020/Hidden_Context_English_Image.npy', allow_pickle=True)\n",
        "estimator = train_img(train_img_features,'out_dir_train_hindi_pro_img',train_features_hindi,input_fn_builder_progressive = True, hidden_context = hidden_context_eng_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwYCT8IXs1ZN",
        "colab_type": "text"
      },
      "source": [
        "#  Evaluation and Hidden Context generation for Hindi Premis and English Hypothesis (Image Included) (Progressive Variant)\n",
        "*   Hidden Context Obtained\n",
        "*   Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pfHkqWrvu-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_batch_size = 3593  # Test Split Size\n",
        "\n",
        "dummy = np.random.randn(Test_batch_size,768)\n",
        "#test_img_features = np.random.randn(50,1024)\n",
        "#evaluate_img_features = np.random.randn(450,1024)\n",
        "act_lab, pred_lab = evaluate_and_get_hidden_context_img(1,test_img_features,train_img_features,estimator,input_fn_for_test = test_features_hindi,input_fn_for_hidden = train_features_eng,is_progressive = True,hidden_context=dummy)\n",
        "\n",
        "np.array(act_lab).dump(open('HE_Actual_labels_Progressive_Image.npy', 'wb'))\n",
        "np.array(pred_lab).dump(open('HE_Predicted_labels_Progressive_Image.npy', 'wb'))\n",
        "\n",
        "y_true = list(np.load('HE_Actual_labels_Progressive_Image.npy', allow_pickle=True))\n",
        "y_pred = list(np.load('HE_Predicted_labels_Progressive_Image.npy', allow_pickle=True))\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment','Other']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
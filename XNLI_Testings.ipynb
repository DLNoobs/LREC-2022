{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XNLI Testings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BzqiIMgJrq-w",
        "jxarj7wkrq5j"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPy5uglhqQYQ",
        "colab_type": "text"
      },
      "source": [
        "*   **Use iitp.baban Google Drive**\n",
        "*   **SEED = 42** \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1eNCKuMqV_S",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyZMo5A8t2GK",
        "colab_type": "code",
        "outputId": "326976d0-c192-4bbd-b4c9-2f96280806ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3hC_qnvt5SJ",
        "colab_type": "code",
        "outputId": "815e52c4-c3c7-48b2-ddc3-3ff0ca1e52ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZKm3PjqfKw",
        "colab_type": "text"
      },
      "source": [
        "# BERT Pretrained Model Download "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQsRqD2Ht7JY",
        "colab_type": "code",
        "outputId": "428174b9-2313-440d-c775-19929379c26a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip multi_cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-15 12:15:28--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.214.128, 2607:f8b0:4001:c05::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.214.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M   127MB/s    in 5.1s    \n",
            "\n",
            "2020-06-15 12:15:33 (125 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuKw9eWfrpv2",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loading (Text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxdLFOYAolBH",
        "colab_type": "code",
        "outputId": "2ea99574-ed02-4528-b49a-1355be89040b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
        "!unzip XNLI-1.0.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-15 12:15:47--  https://dl.fbaipublicfiles.com/XNLI/XNLI-1.0.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17865352 (17M) [application/zip]\n",
            "Saving to: ‘XNLI-1.0.zip’\n",
            "\n",
            "XNLI-1.0.zip        100%[===================>]  17.04M  14.7MB/s    in 1.2s    \n",
            "\n",
            "2020-06-15 12:15:49 (14.7 MB/s) - ‘XNLI-1.0.zip’ saved [17865352/17865352]\n",
            "\n",
            "Archive:  XNLI-1.0.zip\n",
            "   creating: XNLI-1.0/\n",
            "  inflating: XNLI-1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/XNLI-1.0/\n",
            "  inflating: __MACOSX/XNLI-1.0/._.DS_Store  \n",
            "  inflating: XNLI-1.0/xnli.dev.tsv   \n",
            "  inflating: __MACOSX/XNLI-1.0/._xnli.dev.tsv  \n",
            "  inflating: XNLI-1.0/xnli.dev.jsonl  \n",
            "  inflating: XNLI-1.0/README.md      \n",
            "  inflating: __MACOSX/XNLI-1.0/._README.md  \n",
            "  inflating: XNLI-1.0/xnli.test.jsonl  \n",
            "  inflating: XNLI-1.0/xnli.test.tsv  \n",
            "  inflating: __MACOSX/XNLI-1.0/._xnli.test.tsv  \n",
            "  inflating: __MACOSX/._XNLI-1.0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1H4EarpYaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('XNLI-1.0/xnli.test.tsv',sep = '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmt_DkfXqzBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_fr = df[(df['language'] == 'fr') ].head(5000)\n",
        "df_de = df[(df['language'] == 'de') ].head(5000)\n",
        "df_tr = df[(df['language'] == 'tr') ].head(5000)\n",
        "df_bg = df[(df['language'] == 'bg') ].head(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JzsFQYRridp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_fr_train,df_fr_test = train_test_split(df_fr, test_size=0.1,random_state = SEED,shuffle = True)\n",
        "df_de_train,df_de_test = train_test_split(df_de, test_size=0.1,random_state = SEED,shuffle = True)\n",
        "df_tr_train,df_tr_test = train_test_split(df_tr, test_size=0.1,random_state = SEED,shuffle = True)\n",
        "df_bg_train,df_bg_test = train_test_split(df_bg, test_size=0.1,random_state = SEED,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65ZPFUj2t-86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_for_label(a,b,lang_pair):\n",
        "  pairs = lang_pair.split('-')\n",
        "  b_ = list(a['gold_label'])\n",
        "  assert b_== list(b['gold_label'])\n",
        "  lab = []\n",
        "  \"\"\"\n",
        "  lab  = []\n",
        "  for i in b_:\n",
        "    lab.append(i-1)\n",
        "  \"\"\"\n",
        "  for i in b_:\n",
        "    if i=='contradiction':\n",
        "        lab.append(0)\n",
        "        \n",
        "    elif i=='neutral':\n",
        "        lab.append(1)\n",
        "    elif i== 'entailment':\n",
        "        lab.append(2)\n",
        "    \n",
        "  sentence_1 = list(a['sentence1'])\n",
        "  sentence_2 = list(b['sentence2'])\n",
        "  raw_data_train = {'sentence1_{}'.format(pairs[0]): sentence_1, \n",
        "              'sentence2_{}'.format(pairs[1]): sentence_2,\n",
        "          'label': lab}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['sentence1_{}'.format(pairs[0]),'sentence2_{}'.format(pairs[1]),'label'])\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1afxHh59QYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select Language Pair for Training\n",
        "train_data = get_data_for_label(df_fr_train,df_tr_train,'fr-tr')\n",
        "test_data =  get_data_for_label(df_fr_test,df_tr_test,'fr-tr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn_yth2r9Mxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(a,b,lang_pair):\n",
        "  pairs = lang_pair.split('-')\n",
        "  b_ = list(a['gold_label'])\n",
        "  assert b_== list(b['gold_label'])\n",
        "  lab = []\n",
        "  \"\"\"\n",
        "  lab  = []\n",
        "  for i in b_:\n",
        "    lab.append(i-1)\n",
        "  \"\"\"\n",
        "  for i in b_:\n",
        "    if i=='contradiction':\n",
        "        lab.append(0)\n",
        "        \n",
        "    elif i=='neutral':\n",
        "        lab.append(1)\n",
        "    elif i== 'entailment':\n",
        "        lab.append(2)\n",
        "    \n",
        "  sentence_1 = list(a['sentence1'])\n",
        "  sentence_2 = list(b['sentence2'])\n",
        "  raw_data_train = {'sentence1_{}'.format(pairs[0]): sentence_1, \n",
        "              'sentence2_{}'.format(pairs[1]): sentence_2,\n",
        "          'label': lab}\n",
        "  df = pd.DataFrame(raw_data_train, columns = ['sentence1_{}'.format(pairs[0]),'sentence2_{}'.format(pairs[1]),'label'])\n",
        "  return df\n",
        "\n",
        "def get_features(lang_pair):\n",
        "  features = {}\n",
        "  \n",
        "  features[lang_pair+'_train'] = 0\n",
        "  features[lang_pair+'_test'] = 0\n",
        "  lang_dict = {'fr':[df_fr_train,df_fr_test],'de':[df_de_train,df_de_test],'tr':[df_tr_train,df_tr_test],'bg':[df_bg_train,df_bg_test]}\n",
        "  pairs = lang_pair.split('-')\n",
        "  lang1_train,lang2_train = lang_dict[pairs[0]][0],lang_dict[pairs[1]][0]\n",
        "  lang1_test,lang2_test = lang_dict[pairs[0]][1],lang_dict[pairs[1]][1]\n",
        "  features[lang_pair+'_train'] = get_data(lang1_train,lang2_train,lang_pair)\n",
        "  features[lang_pair+'_test'] = get_data(lang1_test,lang2_test,lang_pair)\n",
        "  features[lang_pair+'_train'] = features[lang_pair+'_train'].apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['sentence1_{}'.format(pairs[0])], \n",
        "                                                                   text_b = x['sentence2_{}'.format(pairs[1])], \n",
        "                                                                   label = x['label']), axis = 1)\n",
        "  features[lang_pair+'_test'] = features[lang_pair+'_test'].apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x['sentence1_{}'.format(pairs[0])], \n",
        "                                                                   text_b = x['sentence2_{}'.format(pairs[1])], \n",
        "                                                                   label = x['label']), axis = 1)\n",
        "  vocab_file = \"multi_cased_L-12_H-768_A-12/vocab.txt\"\n",
        "  label_list = [0,1,2]\n",
        "  tokenizer = bert.tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=True)\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "\n",
        "  features[lang_pair+'_train'] = bert.run_classifier.convert_examples_to_features(features[lang_pair+'_train'], label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  features[lang_pair+'_test'] = bert.run_classifier.convert_examples_to_features(features[lang_pair+'_test'], label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TeXRX6Ajm_1",
        "colab_type": "code",
        "outputId": "098903e6-f3f0-404f-e225-a305929b1ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "features = get_features('fr-tr')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] vous com ##pre ##nez deja l ' importance de la narra ##tion , de la po ##esie , de la chanson et du theatre pour sus ##cite ##r l ' empat ##hie , la com ##pass ##ion et l ' ima ##gination . [SEP] o ##yku an ##lat ##ma , si ##ir , sa ##rk ##ı ve ti ##yatro ve empat ##i kur ##ma , mer ##ham ##et ve haya ##l gu ##cun ##un te ##s ##vik edi ##lmesi ##nde kes ##in ##likle one ##ms ##iz ##dir ve ise ya ##rama ##z . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 24931 10212 30619 38087 46624 180 112 21912 10104 10109 63335 10822 117 10104 10109 10514 86258 117 10104 10109 21664 10131 10168 28016 10322 10846 46963 10129 180 112 30593 72287 117 10109 10212 36388 11046 10131 180 112 13872 69428 119 102 183 83496 10151 16698 10369 117 10294 10835 117 10148 16299 10713 10323 14382 73279 10323 30593 10116 19830 10369 117 13697 13196 10308 10323 57069 10161 75980 48917 11107 10361 10107 21533 105861 80008 11382 21388 10245 92067 10464 12387 15834 11957 10323 14680 10549 46582 10305 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] . . . les e ##cri ##vai ##ns sur la nature les plus attention ##nes et les plus motiv ##ants au monde . [SEP] hi ##c kim ##se dog ##ay ##la ilgili ya ##zı ya ##zm ##ı ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 119 119 119 10152 173 99590 37557 10891 10326 10109 16613 10152 10563 21341 11496 10131 10152 10563 63598 22595 10257 13111 119 102 11520 10350 38516 10341 17835 13998 10330 48009 10549 44047 10549 37661 10713 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] eu ##h , non , pour etre hon ##nete , je n ' ai jamais lu aucun des livres que j ' eta ##is su ##ppo ##se lire . [SEP] 100 say ##fa ##dan daha uzun hi ##c ##bir kita ##p ok ##uma ##dı ##m . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 14444 10237 117 10446 117 10322 43789 14923 51022 117 10144 182 112 11346 22168 14657 30366 10139 28285 10121 178 112 10408 10291 10198 45565 10341 34944 119 102 10407 23763 13369 12146 13878 31975 11520 10350 29241 40091 10410 14302 16746 17532 10147 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] tu lui accord ##es trop d ' importance . [SEP] bir se ##y ya ##rat ##ı ##yor ##sun . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13055 10830 35233 10171 27830 172 112 21912 119 102 10561 10126 10157 10549 12553 10713 26101 24883 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] tous deux peuvent etre modi ##fies sans que le me ##cani ##sme de correspond ##ance entre antico ##don et co ##don soit ega ##lement modi ##fie . [SEP] kar ##sı ##t diz ##gi - diz ##gi es ##les ##tir ##me me ##kan ##izma ##sı de ##gist ##iri ##lme ##den bun ##ların hi ##c ##bir ##i de ##gist ##iri ##lemez . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13477 11051 18715 43789 92734 63199 13115 10121 10141 10911 108944 17228 10104 42996 16460 10402 36232 15081 10131 11170 15081 14583 45192 37587 92734 31185 119 102 25085 13836 10123 36897 11210 118 36897 11210 10196 11268 18330 10627 10911 10706 84063 13836 10104 77362 19334 53048 10633 59230 15628 11520 10350 29241 10116 10104 77362 19334 94168 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Writing example 0 of 500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] comment un parent peut - il ide ##nti ##fier la difference entre un trouble du langage et le develop ##pe ##ment normal du langage ? [SEP] bir e ##be ##vey ##n , iki ya ##sında ##ki co ##cu ##kla konu ##sama ##manı ##n ano ##rmal oldu ##gun ##u dus ##unu ##r . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 49641 10119 43045 12568 118 10154 38938 12752 28420 10109 30856 10402 10119 58285 10168 67052 10131 10141 26800 11355 10426 16626 10168 67052 136 102 10561 173 11044 50038 10115 117 12322 10549 25666 10506 11170 12352 22444 98881 62009 68455 10115 12797 66619 17312 32657 10138 20239 28041 10129 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] selon un representant du com ##ite , les certifications 605 ( b ) de ce dernier n ' ont pas et ##e trans ##mises se ##pare ##ment au conseiller en chef pour la promotion au sein de la small business administration ( administration des petites entreprises - s ##ba ) . [SEP] kuru ##l , s ##ba sert ##if ##ikal ##arın ##ı ver ##medi ve bunu de ##ger ##lendi ##rici ##nin of ##isin ##e b ##ır ##akt ##ı . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 14944 10119 67698 10168 10212 12704 117 10152 92587 48141 113 170 114 10104 10794 16151 182 112 11378 10801 10131 10112 37241 81282 10126 28927 10426 10257 39396 10110 15158 10322 10109 23175 10257 11479 10104 10109 12474 14155 17941 113 17941 10139 32181 37717 118 187 10537 114 119 102 48350 10161 117 187 10537 38005 13918 52564 88638 10713 16719 96092 10323 79471 10104 11446 72384 30558 11802 10108 33560 10112 170 17145 45522 10713 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] enfin , la dens ##ite postal ##e semble etre un fact ##eur plus important des co ##uts unit ##aires de liv ##rais ##on dans la rue que le volume par rapport aux four ##chet ##tes re ##elles en franc ##e et aux eta ##ts - uni ##s . [SEP] posta yo ##gun ##lug ##u ul ##ast ##ır ##ma mali ##yet ##lerinin buy ##uk ##lug ##une iki kat etki edi ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 37063 117 10109 37144 12704 30715 10112 29264 43789 10119 18638 12986 10563 12452 10139 11170 33876 16511 29194 10104 25585 35885 10263 10260 10109 16505 10121 10141 15901 10248 17508 10754 11598 42298 11197 11639 35199 10110 63184 10112 10131 10754 10408 10806 118 69191 10107 119 102 29175 13672 32657 67381 10138 16600 15171 17145 10369 38981 33878 31268 47715 13013 67381 19659 12322 27689 47033 105861 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] selon lui , toute forme de pense ##e super ##ieur ##e app ##ara ##it d ' abord dans la communication sociale , entre l ' enfant et les representant ##s de sa culture dans une act ##ivi ##te commune . [SEP] ortak aktivite ##leri pay ##las ##mak daha us ##t du ##zey dus ##un ##mele ##ri pay ##las ##mak ici ##n yardım ##cı ol ##uy ##or . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 14944 10830 117 16920 14188 10104 77184 10112 25212 57457 10112 72894 12677 10486 172 112 22913 10260 10109 24990 19279 117 10402 180 112 29581 10131 10152 67698 10107 10104 10148 15162 10260 10231 19833 22317 10216 11380 119 102 95045 39720 12538 16868 13983 23892 13878 19626 10123 10168 68060 20239 11107 81612 10401 16868 13983 23892 42032 10115 79346 18311 30668 53452 10667 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] et ca m ' a encore fait peur [SEP] sadece bir ##az ##cı ##k kor ##km ##ust ##um . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10131 11135 181 112 169 14375 11329 90908 102 39958 10561 16724 18311 10174 33705 23440 19265 10465 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccNTm7l5j-KT",
        "colab_type": "code",
        "outputId": "62371482-7d75-4a97-9ca5-6a7f5a5e834c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "features.keys()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['fr-tr_train', 'fr-tr_test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzqiIMgJrq-w",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-BERT Custom Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVk6Z7w4uo8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels,output_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels,hidden_context) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "       \n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels, \"hidden_context\": hidden_context})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxarj7wkrq5j",
        "colab_type": "text"
      },
      "source": [
        "# CLTE-Progressive-BERT Custom Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESrCwHX2usTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_progressive(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings,hidden_context):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = bert.run_classifier.modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  # In the demo, we are doing a simple classification task on the entire\n",
        "  # segment.\n",
        "  #\n",
        "  # If you want to use the token-level output, use model.get_sequence_output()\n",
        "  # instead.\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "\n",
        "    output_layer_probs = tf.nn.softmax(output_layer,axis = -1)\n",
        "    #loss = y_true * log(y_true / y_pred)\n",
        "    hidden_context = tf.nn.softmax(hidden_context,axis = -1)\n",
        "    per_example_kd_loss = tf.keras.losses.KLD(hidden_context,output_layer_probs)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "\n",
        "    kd_loss_weight = 0.5 #hyperparameter\n",
        "    per_example_kd_loss = kd_loss_weight*per_example_kd_loss\n",
        "\n",
        "    per_example_loss += per_example_kd_loss\n",
        "\n",
        "    \n",
        "\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities,predicted_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_fn_builder_progressive(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    hidden_context = features[\"hidden_context\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities,predicted_labels) = create_model_progressive(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings,hidden_context)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = bert.run_classifier.modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    \"\"\"\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    \"\"\"\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = metric_fn(per_example_loss, label_ids, logits, is_real_example)\n",
        "      \n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metric_ops=eval_metrics)\n",
        "    else:\n",
        "      output_spec = tf.estimator.EstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities,\"labels\": predicted_labels})\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uNc6dcsrqia",
        "colab_type": "text"
      },
      "source": [
        "# Input Functions\n",
        "\n",
        "1.   CLTE-BERT\n",
        "2.   CLTE-BERT with Image\n",
        "3.   CLTE-BERT-Progressive with Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n8-XpQ5uxFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features, hidden_context,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape = hidden_context.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"hidden_context\":\n",
        "            tf.constant(hidden_context, shape = [num_examples,hidden_shape], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "\n",
        "def input_fn_builder_img(img_features,features,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape_img = img_features.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"img_features\":\n",
        "            tf.constant(img_features, shape = [num_examples,hidden_shape_img], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def input_fn_builder_pr_img(img_features,features,hidden_context,seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "    hidden_shape_img = img_features.shape[-1]\n",
        "    hidden_shape = hidden_context.shape[-1]\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "\n",
        "        \"img_features\":\n",
        "            tf.constant(img_features, shape = [num_examples,hidden_shape_img], dtype = tf.float32),\n",
        "\n",
        "        \"hidden_context\":\n",
        "            tf.constant(hidden_context, shape = [num_examples,hidden_shape], dtype = tf.float32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJqVRXAurqgj",
        "colab_type": "text"
      },
      "source": [
        "# Trainer Functions for BERT (With and Without Image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trvxhdrguz2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 5 # Number of Training Epochs \n",
        "\n",
        "\n",
        "def train(output_dir,input_fn,input_fn_builder_progressive = False,hidden_context = None):\n",
        "  CONFIG_FILE = \"multi_cased_L-12_H-768_A-12/bert_config.json\"\n",
        "  INIT_CHECKPOINT = \"multi_cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "  BATCH_SIZE = 28\n",
        "  LEARNING_RATE = 2e-5\n",
        "  NUM_TRAIN_EPOCHS = Epochs\n",
        "  # Warmup is a period of time where hte learning rate \n",
        "  # is small and gradually increases--usually helps training.\n",
        "  WARMUP_PROPORTION = 0.1\n",
        "  # Model configs\n",
        "  SAVE_CHECKPOINTS_STEPS = 15000\n",
        "  SAVE_SUMMARY_STEPS = 100\n",
        "  OUTPUT_DIR = output_dir\n",
        "  # Compute # train and warmup steps from batch size\n",
        "  num_train_steps = int(len(input_fn) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "  print(num_train_steps)\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "  # Specify outpit directory and number of checkpoint steps to save\n",
        "  if input_fn_builder_progressive==False:\n",
        "  \n",
        "\n",
        "\n",
        "    model_fn = model_fn_builder(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "  \n",
        "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "        features=input_fn,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "\n",
        "  else:\n",
        "\n",
        "    model_fn_pr = model_fn_builder_progressive(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn_pr,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "    train_input_fn = input_fn_builder(\n",
        "        features=input_fn,\n",
        "        hidden_context=hidden_context,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "  print(f'Beginning Training!')\n",
        "  %timeit\n",
        "\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  return estimator\n",
        "\n",
        "\n",
        "def train_img(img_features,output_dir,input_fn,input_fn_builder_progressive = False,hidden_context = None):\n",
        "  CONFIG_FILE = \"multi_cased_L-12_H-768_A-12/bert_config.json\"\n",
        "  INIT_CHECKPOINT = \"multi_cased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "\n",
        "  BATCH_SIZE = 28\n",
        "  LEARNING_RATE = 2e-5\n",
        "  NUM_TRAIN_EPOCHS = Epochs              \n",
        "  # Warmup is a period of time where hte learning rate \n",
        "  # is small and gradually increases--usually helps training.\n",
        "  WARMUP_PROPORTION = 0.1\n",
        "  # Model configs\n",
        "  SAVE_CHECKPOINTS_STEPS = 15000\n",
        "  SAVE_SUMMARY_STEPS = 100\n",
        "  OUTPUT_DIR = output_dir\n",
        "  # Compute # train and warmup steps from batch size\n",
        "  num_train_steps = int(len(input_fn) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "  num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "  print(num_train_steps)\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      model_dir=OUTPUT_DIR,\n",
        "      save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
        "\n",
        "  # Specify outpit directory and number of checkpoint steps to save\n",
        "  if input_fn_builder_progressive==False:\n",
        "  \n",
        "\n",
        "\n",
        "    model_fn = model_fn_builder_img(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "  \n",
        "    train_input_fn = input_fn_builder_img(\n",
        "        img_features = img_features,\n",
        "        features=input_fn,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "    \n",
        "\n",
        "  else:\n",
        "\n",
        "    model_fn_pr = model_fn_builder_img_progressive(\n",
        "      bert_config=bert.run_classifier.modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "      num_labels=4, #number of unique labels\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=False,\n",
        "      use_one_hot_embeddings=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    estimator = tf.estimator.Estimator(\n",
        "      model_fn=model_fn_pr,\n",
        "      config=run_config,\n",
        "      params={\"batch_size\": BATCH_SIZE})\n",
        "\n",
        "  \n",
        "    train_input_fn = input_fn_builder_pr_img(\n",
        "        img_features = img_features,\n",
        "        features=input_fn,\n",
        "        hidden_context=hidden_context,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=False)\n",
        "\n",
        "  print(f'Beginning Training!')\n",
        "  %timeit\n",
        "\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNzleH_urqeW",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Functions for BERT (With and Without Image)\n",
        "\n",
        "*   CTX = 0 for English Premise and Hindi Hypothesis\n",
        "*   CTX = 1 for Hindi Premise and English Hypothesis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbhGSEfgu2DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def evaluate_and_get_hidden_context(estimator,input_fn_for_test,input_fn_for_hidden,is_progressive = False,hidden_context=None):\n",
        "  MAX_SEQ_LENGTH = 128\n",
        "  if not is_progressive:\n",
        "    test_input_fn = run_classifier.input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    actual_labels = []\n",
        "    for i in test_data['label']:\n",
        "      actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "    print(f'acc {accuracy_score(actual_labels,predicted_labels)} ')\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    hidden_input_fn = run_classifier.input_fn_builder(\n",
        "        features=input_fn_for_hidden,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=hidden_input_fn, steps=None)\n",
        "    res_ = estimator.predict(hidden_input_fn)\n",
        "    hidden_context = []\n",
        "    k = 0\n",
        "    try:\n",
        "      for i in res_:\n",
        "        #print(i['hidden_context'])\n",
        "        \n",
        "        hidden_context.append(i[\"hidden_context\"])\n",
        "        k+=1\n",
        "    except:\n",
        "      print(f'k is {k}')\n",
        "    hidden_context = np.array(hidden_context)\n",
        "    return hidden_context,actual_labels,predicted_labels\n",
        "  else:\n",
        "    test_input_fn = input_fn_builder(\n",
        "      features=input_fn_for_test,\n",
        "      hidden_context=hidden_context,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=False)\n",
        "    estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
        "    actual_labels = []\n",
        "    for i in test_data['label']:\n",
        "      actual_labels.append(i)\n",
        "\n",
        "    res = estimator.predict(test_input_fn)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for i in res:\n",
        "      predicted_labels.append(i['labels'])\n",
        "    return actual_labels,predicted_labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjdlPxj0s1wD",
        "colab_type": "text"
      },
      "source": [
        "# Progressive Training on Chosen Premise and Hypothesis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFQdoMANvEq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f41087cb-fa0a-4739-b5d9-2193c814e4f9"
      },
      "source": [
        "#get hidden context from drive\n",
        "hidden_context_data = np.load('/content/gdrive/My Drive/XNLI Hidden Contexts/Hidden_Context_fr-de_Normal.npy', allow_pickle=True)\n",
        "#get features using get_features\n",
        "features = get_features('fr-tr')\n",
        "estimator = train('out_dir_train_eng_pro', features['fr-tr_train'], input_fn_builder_progressive = True, hidden_context = hidden_context_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] vous com ##pre ##nez deja l ' importance de la narra ##tion , de la po ##esie , de la chanson et du theatre pour sus ##cite ##r l ' empat ##hie , la com ##pass ##ion et l ' ima ##gination . [SEP] o ##yku an ##lat ##ma , si ##ir , sa ##rk ##ı ve ti ##yatro ve empat ##i kur ##ma , mer ##ham ##et ve haya ##l gu ##cun ##un te ##s ##vik edi ##lmesi ##nde kes ##in ##likle one ##ms ##iz ##dir ve ise ya ##rama ##z . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 24931 10212 30619 38087 46624 180 112 21912 10104 10109 63335 10822 117 10104 10109 10514 86258 117 10104 10109 21664 10131 10168 28016 10322 10846 46963 10129 180 112 30593 72287 117 10109 10212 36388 11046 10131 180 112 13872 69428 119 102 183 83496 10151 16698 10369 117 10294 10835 117 10148 16299 10713 10323 14382 73279 10323 30593 10116 19830 10369 117 13697 13196 10308 10323 57069 10161 75980 48917 11107 10361 10107 21533 105861 80008 11382 21388 10245 92067 10464 12387 15834 11957 10323 14680 10549 46582 10305 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] . . . les e ##cri ##vai ##ns sur la nature les plus attention ##nes et les plus motiv ##ants au monde . [SEP] hi ##c kim ##se dog ##ay ##la ilgili ya ##zı ya ##zm ##ı ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 119 119 119 10152 173 99590 37557 10891 10326 10109 16613 10152 10563 21341 11496 10131 10152 10563 63598 22595 10257 13111 119 102 11520 10350 38516 10341 17835 13998 10330 48009 10549 44047 10549 37661 10713 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] eu ##h , non , pour etre hon ##nete , je n ' ai jamais lu aucun des livres que j ' eta ##is su ##ppo ##se lire . [SEP] 100 say ##fa ##dan daha uzun hi ##c ##bir kita ##p ok ##uma ##dı ##m . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 14444 10237 117 10446 117 10322 43789 14923 51022 117 10144 182 112 11346 22168 14657 30366 10139 28285 10121 178 112 10408 10291 10198 45565 10341 34944 119 102 10407 23763 13369 12146 13878 31975 11520 10350 29241 40091 10410 14302 16746 17532 10147 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] tu lui accord ##es trop d ' importance . [SEP] bir se ##y ya ##rat ##ı ##yor ##sun . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13055 10830 35233 10171 27830 172 112 21912 119 102 10561 10126 10157 10549 12553 10713 26101 24883 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] tous deux peuvent etre modi ##fies sans que le me ##cani ##sme de correspond ##ance entre antico ##don et co ##don soit ega ##lement modi ##fie . [SEP] kar ##sı ##t diz ##gi - diz ##gi es ##les ##tir ##me me ##kan ##izma ##sı de ##gist ##iri ##lme ##den bun ##ların hi ##c ##bir ##i de ##gist ##iri ##lemez . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 13477 11051 18715 43789 92734 63199 13115 10121 10141 10911 108944 17228 10104 42996 16460 10402 36232 15081 10131 11170 15081 14583 45192 37587 92734 31185 119 102 25085 13836 10123 36897 11210 118 36897 11210 10196 11268 18330 10627 10911 10706 84063 13836 10104 77362 19334 53048 10633 59230 15628 11520 10350 29241 10116 10104 77362 19334 94168 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 0 (id = 0)\n",
            "INFO:tensorflow:Writing example 0 of 500\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] comment un parent peut - il ide ##nti ##fier la difference entre un trouble du langage et le develop ##pe ##ment normal du langage ? [SEP] bir e ##be ##vey ##n , iki ya ##sında ##ki co ##cu ##kla konu ##sama ##manı ##n ano ##rmal oldu ##gun ##u dus ##unu ##r . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 49641 10119 43045 12568 118 10154 38938 12752 28420 10109 30856 10402 10119 58285 10168 67052 10131 10141 26800 11355 10426 16626 10168 67052 136 102 10561 173 11044 50038 10115 117 12322 10549 25666 10506 11170 12352 22444 98881 62009 68455 10115 12797 66619 17312 32657 10138 20239 28041 10129 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] selon un representant du com ##ite , les certifications 605 ( b ) de ce dernier n ' ont pas et ##e trans ##mises se ##pare ##ment au conseiller en chef pour la promotion au sein de la small business administration ( administration des petites entreprises - s ##ba ) . [SEP] kuru ##l , s ##ba sert ##if ##ikal ##arın ##ı ver ##medi ve bunu de ##ger ##lendi ##rici ##nin of ##isin ##e b ##ır ##akt ##ı . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 14944 10119 67698 10168 10212 12704 117 10152 92587 48141 113 170 114 10104 10794 16151 182 112 11378 10801 10131 10112 37241 81282 10126 28927 10426 10257 39396 10110 15158 10322 10109 23175 10257 11479 10104 10109 12474 14155 17941 113 17941 10139 32181 37717 118 187 10537 114 119 102 48350 10161 117 187 10537 38005 13918 52564 88638 10713 16719 96092 10323 79471 10104 11446 72384 30558 11802 10108 33560 10112 170 17145 45522 10713 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] enfin , la dens ##ite postal ##e semble etre un fact ##eur plus important des co ##uts unit ##aires de liv ##rais ##on dans la rue que le volume par rapport aux four ##chet ##tes re ##elles en franc ##e et aux eta ##ts - uni ##s . [SEP] posta yo ##gun ##lug ##u ul ##ast ##ır ##ma mali ##yet ##lerinin buy ##uk ##lug ##une iki kat etki edi ##yor . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 37063 117 10109 37144 12704 30715 10112 29264 43789 10119 18638 12986 10563 12452 10139 11170 33876 16511 29194 10104 25585 35885 10263 10260 10109 16505 10121 10141 15901 10248 17508 10754 11598 42298 11197 11639 35199 10110 63184 10112 10131 10754 10408 10806 118 69191 10107 119 102 29175 13672 32657 67381 10138 16600 15171 17145 10369 38981 33878 31268 47715 13013 67381 19659 12322 27689 47033 105861 26101 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] selon lui , toute forme de pense ##e super ##ieur ##e app ##ara ##it d ' abord dans la communication sociale , entre l ' enfant et les representant ##s de sa culture dans une act ##ivi ##te commune . [SEP] ortak aktivite ##leri pay ##las ##mak daha us ##t du ##zey dus ##un ##mele ##ri pay ##las ##mak ici ##n yardım ##cı ol ##uy ##or . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 14944 10830 117 16920 14188 10104 77184 10112 25212 57457 10112 72894 12677 10486 172 112 22913 10260 10109 24990 19279 117 10402 180 112 29581 10131 10152 67698 10107 10104 10148 15162 10260 10231 19833 22317 10216 11380 119 102 95045 39720 12538 16868 13983 23892 13878 19626 10123 10168 68060 20239 11107 81612 10401 16868 13983 23892 42032 10115 79346 18311 30668 53452 10667 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 2 (id = 2)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: None\n",
            "INFO:tensorflow:tokens: [CLS] et ca m ' a encore fait peur [SEP] sadece bir ##az ##cı ##k kor ##km ##ust ##um . [SEP]\n",
            "INFO:tensorflow:input_ids: 101 10131 11135 181 112 169 14375 11329 90908 102 39958 10561 16724 18311 10174 33705 23440 19265 10465 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: 1 (id = 1)\n",
            "803\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'out_dir_train_eng_pro', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 15000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4b331a02b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "Beginning Training!\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = hidden_context, shape = (?, 768)\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1pYkArxza3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_batch_size = 500\n",
        "dummy = np.random.randn(Test_batch_size,768)\n",
        "act_lab, pred_lab = evaluate_and_get_hidden_context(estimator,input_fn_for_test = features['fr-tr_test'], input_fn_for_hidden = features['fr-tr_train'],is_progressive = True, hidden_context = dummy)\n",
        "\n",
        "#Classification Report\n",
        "target_names = ['Contradiction', 'Neutral', 'Entailment']\n",
        "print(classification_report(act_lab, pred_lab, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}